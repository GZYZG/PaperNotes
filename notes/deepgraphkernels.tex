\begin{center}

  \begin{tabular}{rp{16cm}lp{20cm}}%{rl}

  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...

  论文地址：& \href{https://dl.acm.org/doi/pdf/10.1145/2783258.2783417}{https://dl.acm.org/doi/pdf/10.1145/2783258.2783417} \\
  来源：& KDD’15, Sydney, NSW, Australia \\
  作者：& Pinar Yanardag, S.V.N. Vishwanathan \\
  %源码：& \href{xxx}{xxx} \\

%  slides：& \href{http://yunshengb.com/wp-content/uploads/2017/03/nips_2018_r2l_workshop_talk.pdf}{{\footnotesize Convolutional Set Matching for Graph Similarity}}\\

  关键词：& \textbf{graph kernels, deep learning} \\

  写于：& \date{2020-11-02}

  \end{tabular}

\end{center}

该论文\cite{yanardag2015deep}针对传统的graph kernel方法的缺点提出了Deep Graph Kernels --- 一个能够学习图子结构的潜在表示的统一框架。

\paragraph{使用Graph Kernels计算图相似性}如下公式所示，其中$\phi(\mathcal{G})$表示图$\mathcal{G}$基于子结构的表示向量，例如该向量的每一维表示某一个子结构的出现次数；$\mathcal{M}$表示了所有子结构之间的关系；$\mathcal{K}(G_1, G_2)$是一个核函数，可以用来计算图之间的相似性。
$$
\mathcal{K}(G_1, G_2) = \phi(G_1) \mathcal{M} \phi(G_2)
$$
该论文的重要一点就是学习到子结构的向量表示并计算得到表示这子结构间关系的矩阵$\mathcal{M}$。

传统的graph kernel方法是将图数据（不一定是图，也可以是其他类型的数据，如数、序列等）分解为许多小的子结构，在子结构的基础上就可以将一个图表示为基于子结构的向量。在这些子结构基础上定义有效的核函数（原始特征在高维空间中的内积），就可以使用核函数来计算图之间的相似性。但是传统的graph kernel方法有以下几个缺点：
\begin{itemize}
	\item 通常来说，子结构之间并不是相互独立的。如graphlets（拥有k个结点的，连通的非同构图）。k+1个结点的graphlet包含了k个结点的graphlet
	\item 当图数据很大时，会有很多不同的子结构，图的基于子结构的向量表示将会变得很稀疏。只有一些子结构会出现在图中，在这种情况下，会出现diagonal dominance --- 一个图只与自己相似，与数据集中其他图不相似/相似度很低
\end{itemize}
个人觉得，该论文主要通过神经语言模型的方法学习到子结构的表示，再方便地计算$\mathcal{M}$。但是论文中说的有点含糊。

简单的介绍以下Graph kernels。有很多中Graph kernel，比如基于graphlets的、基于subtree的、基于最短路径的等等，但除了如何生成子结构的方法不同，它们都会用子结构的频数向量来作为图的表示。graph kernels的方法很像language model中的一些方法，如word2vec中的方法。

\paragraph{思路}论文针对不同的Graph kernels 提出了一个统一的框架，能依据子结构之间的依赖性学习子结构的向量表示（即隐表示）。学习隐表示的方法借鉴了神经语言模型中的概念 --- 使用连续的向量表示词使用上下文来定义word。在该论文中，word就是graph kernel，每一个图可以视作sentence，用多个graph kernels即可表示一个图。论文中针对不同的graph kernels方法，使用不同的方法将图分解为子结构的集合。但与语言模型不同的是，一个图生成的子结构间并没有一个明显的共线关系，因此论文中针对不同的graph kernels给出了不同的解决方法 --- 使用不同的方法体现出子结构之间的共现性（也即子结构之间的依赖关系、上下文）。具体怎么产生子结构之间的共现性就不一样描述了，重点是根据graph kernel的特点，衍生出相关的子结构。在生成了子结构以其之间的共现关系后，就可以使用CBOW/Skip-gram算法来学习子结构的向量表示了。之后便可借助子结构的向量表示来计算图之间的相似性了。


\paragraph{方法解决的问题/优势}
\begin{itemize}
	\item 提出了一个统一的框架，能够在不同的graph kernels下学习到子结构的隐表示
	\item 利用graph kernels之间的依赖关系来定义子结构的共现性
\end{itemize}

\paragraph{方法的局限性/未来方向}
\begin{itemize}
	\item 对更复杂的图数据的graph kernels的定义和学习，如attributed graphs
	\item graph kernels方法主要关注的是图的结构信息，不适用于很多真实的图数据
\end{itemize}